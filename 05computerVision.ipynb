{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq2o9EST11yp"
   },
   "source": [
    "# Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <strong>Name: </strong> [ Write your surname.name between the brackets (like that surname.name) ]  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iemMW2yH1XIs"
   },
   "source": [
    "#### Welcome to this exercise which introduces Computer Vision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWmnjDZB2gxZ"
   },
   "source": [
    "<img style=\"float:right;\" src=\"./img/pinhole_model.png\" width= 50% />\n",
    "\n",
    "<br>\n",
    "While photogrammetry has over 100 years of tradition, the past several years has seen Computer Vision applied in the geospatial domain.\n",
    "<br> <br>\n",
    "\n",
    "\n",
    "Newer technologies often come with their own jargon (terms, definitions and language). \n",
    "\n",
    "This exercise is not meant to be exhaustive but serves as a gentle introduction to the concepts and potential of Computer Vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lBqbY1r2g1h"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> <strong> Lets first define the core difference between these two methods. </strong>  <br> <br>\n",
    "\n",
    "**Computer Vision**: Involves the interpretation and understanding of visual data often involving real-time decision-making by machines. Enable machines to interpret and understand visual data. _Medical imaging and autonomous navigation_  \n",
    "**Photogrammetry**: reconstruct 3D structures and spatial relationships. Accurately measure and reconstruct the three-dimensional shape. _Remote sensing and mapping_</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order for realise the goal of Computer Vision it is often necessary to harvest 3D data from imagery**. While this is also the aim of photogrammetry; the processes differ slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Aspect|Traditional Photogrammetry|Computer Vision|\n",
    "|---|---|---|\n",
    "|Stereo|Primarily relies on **_stereo matching_** to create 3D models|Combines stereo vision with **_advanced techniques_** like Structure from Motion Multi-view Stereo (SfM-MVS) and SLAM (Simultaneous Localization and Mapping)|\n",
    "|Calibration|**_Precise sensor calibration_** for accurate results|May involve more **_automated_** calibration procedures|\n",
    "|Processing Speed|Time-consuming manual processes|Often performs real-time or **_near-real-time processing_** thanks to powerful hardware and **_optimized algorithms_**.|\n",
    "|Data Volume|Deals with smaller datasets of images.|Analyzes large datasets of images, videos, and 3D point clouds.|\n",
    "|Data Availability|Data acquisition and processing may be costly and time-consuming.|Access to data is becoming more abundant and affordable, thanks to the proliferation of digital cameras and sensors|\n",
    "|Accuracy|High accuracy _(or rather the **confidence** in the quality)_ making it suitable for geospatial applications|Accuracy varies depending on the application and the quality of data and algorithms|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBq0MF5L2g5J"
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "  <strong>REQUIRED!</strong> \n",
    "  \n",
    "You are required to insert your outputs and any comment into this document. \n",
    "    \n",
    "The **aim** of this exercise is for you to execute this Notebook on **_a series of photographs you have captured yourself_**. The document you submit should therefore contain the existing text in addition to:   \n",
    "        \n",
    "\n",
    " - Plots and other outputs from exec the code cells  \n",
    " - Discussion of your plots and other outputs as well as conclusions reached.  \n",
    " - This should also include any hypotheses and assumptions made as well as factors that may affect your conclusions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLOmuPSmzwTx"
   },
   "outputs": [],
   "source": [
    "#- load the magic\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#-- this code is mostly from: https://github.com/FlagArihant2000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxXhXIVxvWIh",
    "outputId": "c24ac9c4-b165-4f59-c172-c587850cf492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#- if you find it challenging to install and import the libraries on a local machine; give colab a go. \n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ov2ly6SsxS3Q"
   },
   "outputs": [],
   "source": [
    "#- path (main folder)\n",
    "dir = os.path.join(\".../collinearity/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <strong> For this exercise we will harvest 3D data from imagery with Computer Vision. </strong>  \n",
    "<br><br>\n",
    "We will do so in twice. \n",
    "<br>  \n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1. Traditional stereo-vision** _(two photographs)_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2. Structure-from-Motion (SfM)**\n",
    "<!-- &nbsp;&nbsp;&nbsp;&nbsp;**2.** extend the SfM process with **Multi-view Stereo (MVS)** _(and show how the method accomodates many images)_-->\n",
    "\n",
    "In the process we will work through such concepts as **_Feature Matching, Essential and Fundamental matrices, Disparity, Triangulation and Bundle Adjustment_**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_4FeaOlkkLwl"
   },
   "outputs": [],
   "source": [
    "#- path (to the data)\n",
    "input_dir = os.path.join(dir, 'stereo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stereo-vision\n",
    "\n",
    "Before we _really_ get into **Computer Vision**; it might be helpful to introduce _'algorithm-based'_ stereo photogrammetry. Modern **Computer Vision** is infact already very mature with completely automated workflows that harvest 3D information via Deep Learning _(neural networks)_. \n",
    "\n",
    "We therefore take a step back and start with the basics to understand how these procedures execute a solution. Two images, of the same feature, taken from **_two different viewpoints_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = sorted(os.listdir(input_dir))\n",
    "images = []\n",
    "for img in img_list:\n",
    "    if '.jpg' in img.lower() or '.png' in img.lower():\n",
    "        images = images + [img]\n",
    "#i = 0\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"./data/stereo/im0.png\" alt=\"Drawing\" style=\"width: 550px;\"/> </td>\n",
    "<td> <img src=\"./data/stereo/im0.png\" alt=\"Drawing\" style=\"width: 550px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. a. Feature Matching\n",
    "\n",
    "In order to harvest 3D data we need the location and orientation of the cameras; at the moment the image was captured. \n",
    "\n",
    "Like traditional **Photogrammerty** which needs _**some**_ variables _(ground control, location and orientation of photographs)_ to solve for the **collinearity equation**; **Computer Vision** also needs a few known variables.  \n",
    "\n",
    "Even so; modern **Computer Vision** can recover the geometry and pose of the camera _(the relative orientation)_ from the imagery itself. It can do so through feature detection and matching algorithms which work in two steps:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;i) find features _(known as **keypoints**)_ in images and  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;ii) **match** corresponding features _(connect the features that match and disgard those that don't)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "6H9f-UnTnzMV"
   },
   "outputs": [],
   "source": [
    "#- feature matching\n",
    "def PointMatchingAKAZE(img1, img2, path):\n",
    "\n",
    "    akaze = cv2.AKAZE_create()\n",
    "    kp1, des1 = akaze.detectAndCompute(img1, None)\n",
    "    kp2, des2 = akaze.detectAndCompute(img2, None)\n",
    "\n",
    "    img4 = cv2.drawKeypoints(img1, kp1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite(os.path.join(path, 'keypoints.jpg'), img4)\n",
    "\n",
    "    #- match features\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    #- ratio test\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.70 * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "\n",
    "    #- cv2.drawMatchesKnn expects list of lists as matches.\n",
    "    #img3 = cv2.drawMatches(img1, kp1, img2, kp2, good[:50], None, flags=2)\n",
    "    img3 = cv2.drawMatches(img1, kp1, img2, kp2, good, None, flags=2)\n",
    "    cv2.imwrite(os.path.join(path, 'imL_x_imR.jpg'), img3)\n",
    "\n",
    "    return pts1, pts2, good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**keypoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/stereo/keypoints.jpg\" alt=\"Drawing\" style=\"width: 1000px;\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**matches**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/stereo/imL_x_imR.jpg\" alt=\"Drawing\" style=\"width: 1000px;\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>TASK! </b>  </div>\n",
    "\n",
    "- **You are expected to insert the result** _(the `keypoints` and `imL_x_imR.jpg`)_ **of your own dataset in the `cells`** _(double-click the image and change the file)_ **above and discuss the result.** \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>HINT!</b> Open the image in another application. Zoom and look at the number of features and the respective matches. </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7WYjxQzZn3Ak"
   },
   "outputs": [],
   "source": [
    "#- sift\n",
    "def PointMatchingSIFT(img1, img2, path):\n",
    "    #Matches using SIFT\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp1, des2 = sift.detectAndCompute(img1, None)\n",
    "    kp2, dse2 = sift.detectAndCompute(img2, None)\n",
    "    \n",
    "    img4 = cv2.drawKeypoints(img1, kp1, None)\n",
    "    cv2.imwrite(\"keypoints.jpg\", img4)\n",
    "    \n",
    "    bf = cv2.BFMatcher_create()\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:200], None, flags=2)\n",
    "    cv2.imwrite(os.path.join(path, 'imL_x_imR.jpg'), img3)\n",
    "    \n",
    "    return kp1, kp2, matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>QUESTION! </b>  </div>\n",
    "\n",
    "- **This exercise does not use `Scale-invariant feature transform (SIFT)` to match features but executes the `AKAZE` algorithm** _(you are expected to change the `code` and test both solutions)_.\n",
    "  \n",
    "    **Discuss the difference between these two methods. Mention their strenghts and weaknessess. A note about intellectual property is expected. Your answer cannot be more than 75 words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. b. Camera calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While we can calibrate a camera with `opencv` we use existing calibration information.** \n",
    "\n",
    "You will need to create your own for the **TASK** at the end of this exercise so lets see what it contains.  The `calib.txt` is structured:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "  fx       & 0   & cx   \\\\\n",
    "  0       & fy   & cy   \\\\\n",
    "  0       & 0   & 1     \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "where `fx = f * resX (the number of columns) / sensorSizeX` and `fy = f * resY (the number of rows) / sensorSizeY` with `f = focal length`.  \n",
    "`cx` and `cy` represent the principle point of the image and, with modern digital cameras, can be calculated with `cx = resX (the number of columns) / 2` and `cy = resY (the number of rows) / 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bJqr3Dl7l6Ab"
   },
   "outputs": [],
   "source": [
    "#- calib.\n",
    "def findCalibrationMat():\n",
    "    #with open(input_dir+'intrinsic.txt') as f: #os.path.join(input_dir, 'im3.jpg'))\n",
    "    with open(os.path.join(dir, 'calib.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "    return np.array(\n",
    "        [l.strip().split(' ') for l in lines],\n",
    "        dtype=np.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. c. Fundamental and Essential matrices _(relative orientation)_\n",
    "\n",
    "In **Photogrammetry** we locate and orient _**one**_ image and are able to calculate a 3D coordinate via the **collinearity equation**. When we do intersect **image rays** from more than one image; we do so from already oriented imagery. \n",
    "\n",
    "<img style=\"float:right;\" src=\"./img/e_f-matrices.png\" width= 40% />\n",
    "\n",
    "In order to harvest 3D information; we need to know the relationship between the images (_i.e.: the relative orientation)_.\n",
    "\n",
    "In Computer Vision we call this the **essential and fundamental matrices**. These matrices solve for the geometry and pose of the cameras at the moment the images were captured.\n",
    "\n",
    "The previous step (feature matching) in **Computer Vision** thus serves two purposes. We recover the _relative orientation_ of all images simultaneously (essential matrix) and describe the relationship between images in the same scene. And use these to map points of one image to lines in another (fundamental matrix). \n",
    "\n",
    "Think of an **essential and fundamental matrices** as generalizations of the **collinearity equations** that account for the relative geometry between multiple cameras. They allow for the reconstruction of 3D structures by considering the relationship between corresponding points in multiple cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "efrPSg3Gn7CN"
   },
   "outputs": [],
   "source": [
    "def FindEssentialMat(kp1, kp2, matches, K):\n",
    "    \n",
    "    #- essential matrix\n",
    "    E, mask = cv2.findEssentialMat(kp1, kp2, K, method = cv2.RANSAC, prob = 0.999, threshold = 0.4, mask = None)\n",
    "    kp1 = kp1[mask.ravel() ==1]\n",
    "    kp2 = kp2[mask.ravel() ==1]\n",
    "    #- fundamental matrix\n",
    "    #fundamental_matrix, inliers = cv.findFundamentalMat(kp1, kp2, cv.FM_RANSAC)\n",
    "    \n",
    "    #- essential matrix to rotation and translation\n",
    "    _, R, t, mask = cv2.recoverPose(E, kp1, kp2, K)\n",
    "    \n",
    "    return E, R, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>EXTRA!</b> </div> \n",
    "\n",
    "- **While beyond the scope of this exercise; the overal aim of the Fundamental and Essential matrices is to recover the Epipolar Geometry and the Epipolar Line. These _narrow_ the search space to recovery the Disparity** _(the horizontal difference)_ **between pixels.**\n",
    "\n",
    "  **You are welcome to write 100 words on Epipolar Geometry and Epipolar Line and what it does. The choice is yours. Its up to you.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. d. Disparity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:right;\" src=\"./img/Stereo-Disparity-in-Cameras.png\" width= 40% />\n",
    "<br><br>\n",
    "\n",
    "We see **Disparity** and _perceive depth_. \n",
    "\n",
    "Choose an object 5-meters away from you. Close your left eye... open your left eye.  \n",
    "Now close you right eye. Alternate. Left eye. Right eye.  \n",
    "The object will appear to move _(horizontally)_. left / right. \n",
    "<br><br>\n",
    "We see _**horizontal displacement**_ and interpret the phenomenon as _3D vision_.\n",
    "\n",
    "Modern **Computer Vision** harvests 3D information in _exactly the same_ way as the human eye sees. \n",
    "\n",
    "Because we have correctly oriented photographs we know the _perceived shift in location_ of any feature. We use this knowledge to create a depth _(disparity)_ map that decribes the distance from the camera.\n",
    "<br><br><br><br>\n",
    "We do however need the dimensions (**_width and height_**) of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 2964\n",
    "h = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dMjONpAcoH1E"
   },
   "outputs": [],
   "source": [
    "#def rectifyToDisparity(imgL, imgR, K, D, R, t, path):\n",
    "def rectifyToDisparity(imgL, imgR, K, D, R, t, w, h, path)\n",
    "    \n",
    "    R1, R2, P1, P2, Q, a, b = cv2.stereoRectify(K, D, K, D, (w, h), R, t)\n",
    "    map1, map2 = cv2.initUndistortRectifyMap(K, D, R1, P1, (w, h), cv2.CV_16SC2)\n",
    "    imgLrec = cv2.remap(imgL, map1, map2, cv2.INTER_CUBIC)\n",
    "    #cv2.imwrite(os.path.join(path, 'imgLrectfy.jpg'), imgLrec)\n",
    "    \n",
    "    map3, map4 = cv2.initUndistortRectifyMap(K, D, R2, P2, (w, h), cv2.CV_16SC2)\n",
    "    imgRrec = cv2.remap(imgR, map3, map4, cv2.INTER_CUBIC)\n",
    "    #cv2.imwrite(os.path.join(path, 'imgRrectfy.jpg'), imgRrec)\n",
    "    \n",
    "    max_disparity = 199\n",
    "    min_disparity = 23\n",
    "    num_disparities = max_disparity - min_disparity\n",
    "    window_size = 5\n",
    "    stereo = cv2.StereoSGBM_create(minDisparity = min_disparity, numDisparities = num_disparities, blockSize = 5,\n",
    "                                 uniquenessRatio = 5, speckleWindowSize = 5, speckleRange = 5, disp12MaxDiff = 2,\n",
    "                                 P1 = 8*3*window_size**2, P2 = 32*3*window_size**2)\n",
    "    \n",
    "    stereo2 = cv2.ximgproc.createRightMatcher(stereo)\n",
    "    \n",
    "    lamb = 8000\n",
    "    sig = 1.5\n",
    "    visual_multiplier = 1.0\n",
    "    wls_filter = cv2.ximgproc.createDisparityWLSFilter(stereo)\n",
    "    wls_filter.setLambda(lamb)\n",
    "    wls_filter.setSigmaColor(sig)\n",
    "    \n",
    "    disparity = stereo.compute(imgLrec, imgRrec)\n",
    "    disparity2 = stereo2.compute(imgRrec, imgLrec)\n",
    "    disparity2 = np.int16(disparity2)\n",
    "    \n",
    "    filteredImg = wls_filter.filter(disparity, imgL, None, disparity2)\n",
    "    _, filteredImg = cv2.threshold(filteredImg, 0, max_disparity * 16, cv2.THRESH_TOZERO)\n",
    "    filteredImg = (filteredImg / 16).astype(np.uint8)\n",
    "    cv2.imwrite(os.path.join(path, 'dsprty.jpg'), filteredImg)\n",
    "    \n",
    "    return filteredImg#, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disparity image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- look at the disparity image\n",
    "img = mpimg.imread(input_dir + '/dsprty.jpg')\n",
    "print(repr(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- plot\n",
    "imgplot = plt.imshow(img, cmap=\"Greys_r\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>QUESTION! </b>  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:right;\" src=\"./img/plt_dsprtyPlot.png\" width= 20% />\n",
    "\n",
    "- **Why is the structure of a Disparity image** _(every pixel is a z)_ **exactly the same as a Digital Elevation Model (DEM)? What does this say about what we have just done?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>BONUS!</b> Change the colour palette from gray-scale. </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. e. Project to 3D\n",
    "\n",
    "We are now at the final stage of the stereo-vision process. We want to harvest the values, from the **disparity** map, and convert the _depth_ into 3D information _(a point cloud we can render in a 3D environment)_.\n",
    "\n",
    "In order to do so we need additional information. We need the distance _(the baseline)_ between the cameras (the eyes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = 193.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "07757nREoLK8"
   },
   "outputs": [],
   "source": [
    "#def Reprojection3D(image, disparity, f, b):\n",
    "def Reprojection3D(image, disparity, f, b, w, h, doff):\n",
    "    #- Q. you might need to... \n",
    "    #Q = np.array([[1, 0, 0, -2964/2], [0, 1, 0, -2000/2],[0, 0, 0, f],[0, 0, -1/b, -124.343/b]])\n",
    "    Q = np.array([[1, 0, 0, -w/2], [0, 1, 0, -h/2],[0, 0, 0, f],[0, 0, -1/b, -doff/b]])\n",
    "    \n",
    "    points = cv2.reprojectImageTo3D(disparity, Q)\n",
    "    mask = disparity > disparity.min()\n",
    "    colors = image\n",
    "    \n",
    "    out_points = points[mask]\n",
    "    out_colors = image[mask]\n",
    "    \n",
    "    verts = out_points.reshape(-1,3)\n",
    "    colors = out_colors.reshape(-1,3)\n",
    "    verts = np.hstack([verts, colors])\n",
    "    \n",
    "    ply_header = '''ply\n",
    "        format ascii 1.0\n",
    "        element vertex %(vert_num)d\n",
    "        property float x\n",
    "        property float y\n",
    "        property float z\n",
    "        property uchar blue\n",
    "        property uchar green\n",
    "        property uchar red\n",
    "        end_header\n",
    "        '''\n",
    "    \n",
    "    with open(input_dir + '/stereo.ply', 'w') as f:\n",
    "        f.write(ply_header %dict(vert_num = len(verts)))\n",
    "        np.savetxt(f, verts, '%f %f %f %d %d %d')\n",
    "        \n",
    "def StereoCV(w, h, b):\n",
    "    #- the function\n",
    "    img1 = cv2.imread(os.path.join(input_dir, 'im0.png'))\n",
    "    img2 = cv2.imread(os.path.join(input_dir, 'im1.png'))\n",
    "\n",
    "    pts1, pts2, matches = PointMatchingAKAZE(img1, img2, path = input_dir)\n",
    "\n",
    "    K = findCalibrationMat()\n",
    "    #K = np.array([[3979.911, 0, 1369.115], [0, 3979.911, 1019.507], [0, 0, 1]], dtype = np.float32)\n",
    "    D = np.zeros((5,1), dtype = np.float32)\n",
    "\n",
    "    E, R, t = FindEssentialMat(pts1, pts2, matches, K)\n",
    "\n",
    "    P1 = np.zeros((3,4))\n",
    "    P1 = np.matmul(K, P1)\n",
    "    P2 = np.hstack((R, t))\n",
    "    P2 = np.matmul(K, P2)\n",
    "\n",
    "    #filteredImg, Q = rectifyToDisparity(img1, img2, K, D, (2964, 2000), R, t, input_dir)\n",
    "    #filteredImg, Q = rectifyToDisparity(img1, img2, K, D, R, t, input_dir)\n",
    "    filteredImg, Q = rectifyToDisparity(img1, img2, K, D, R, t, w, h, input_dir)\n",
    "\n",
    "    f = K[0][0]/2\n",
    "    baseline = b/2\n",
    "    doff = (baseline / f) * w # horizontal disparity or shift between the optical centers of the left and right cameras in terms of pixels\n",
    "    #Reprojection3D(img1, filteredImg, f, baseline)\n",
    "    Reprojection3D(img1, filteredImg, f, baseline, w, h, doff)\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sA8F0iPfoLN0"
   },
   "outputs": [],
   "source": [
    "#- execute\n",
    "StereoCV(w, h, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/stereo/result.png\" alt=\"Drawing\" style=\"width: 1000px;\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>TASK! </b>  </div>\n",
    "\n",
    "- **Capture two photographs of any object and execute this exercise. You will need to determine the distance between the cameras** _(baseline)_ **and create a `calib.txt`.**\n",
    "\n",
    "   **In no more than 50 words; discuss the result** _(the stereo.ply)_. **You are expected to comment on the quality, its potential use and how the solution can be improved**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "  <strong>REQUIRED!</strong> \n",
    "  \n",
    "Don't forget. Besides the contents on this Notebook your project folders need to be submitted also.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGsphZNOoLWM"
   },
   "source": [
    "## 2. Structure-from-Motion (SfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:right;\" src=\"./img/global_sfm.png\" width= 50% />\n",
    "<br>\n",
    "\n",
    "**Structure-from-Motion** (SfM) has had an enormous impact on the geospatial community and radically transformed the types of products the geomatics practitioner **_serves clients_**. \n",
    "\n",
    "These range from foundational datasets (Orthomosaic and elevation models) and include value-added products such as immersive 3D environments.\n",
    "<br><br>\n",
    "The current trajectory of technology and demand for more interactive realistic 3D representations of the world (which is a key geomatics skill) means a basic understanding of how these products are created is a decisive advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <strong> In the second part of this exercise we will work-through a typical SfM pipeline. </strong>  \n",
    "<br><br>\n",
    "    \n",
    "**The aim** is to equip the user with the necessary knowledge to understand the underlying process; readily available geospatial software executes at the click of a button. \n",
    "<br><br>\n",
    "We will also revisit and strengthen our knowledge of feature matching, triangulation (space intersection) and bundle adjustment. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From **Part 1.** of this exercise we have seem that we are able to recover the _**relative orientation**_ (pose and geometry of the cameras: the **essential and fundamental matrices**) and **keypoints that match** across images. \n",
    "\n",
    "This is essentially the goal of **SfM**. The orientation of the cameras, at the moment the image was captured, and a sparse point cloud of tiepoints (keypoints) that represent matching features. **SfM** is the preparation before dense reconstruction with **Multi-view Stereo**.\n",
    "\n",
    "Unlike **photogrammetry**, the **SfM** approach does not require prior knowledge (camera location and orientation, or control point information). **SfM** also goes beyond executing the solution on two images (stereo-vision) but is geared towards **_solving massive datasets simultaneously_**. And while essential for geomatics applications; scaling and transformation, from an arbritary relative coordinate system to a local coordinate system, are not necessary but can be introduced later if desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>The typical SfM pipeline executes as follows: </b> \n",
    "<br><br>    \n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**a) Feature matching**;  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**b)** Estimate the **essential matrix and recover the geometry and pose** (rotation and translation) of the camera;   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**c) Triangulation** _(of 2D points into a 3D world)_;   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i) with their respective reprojection errors;  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**d) Refinement** of the rotation and translation parameters (above) **via PnP**;  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**e) Iteratively add a new image into the pipeline**; _feature match, estimate pose, triangulate, refine pose_ and  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**f) a** final least squares **bundle adjustment**.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- path (to the data)\n",
    "input_dir = os.path.join(dir, 'sfm')\n",
    "output_dir = os.path.join(dir, 'sfm', 'result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = sorted(os.listdir(input_dir))\n",
    "images = []\n",
    "for img in img_list:\n",
    "    if '.jpg' in img.lower() or '.png' in img.lower():\n",
    "        images = images + [img]\n",
    "i = 0\n",
    "images.sort()\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. Feature Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointMatchingOpticalFlow(img1, img2, path, filename1 = None, filename2 = None):\n",
    "    # Initialize the FAST detector and BRIEF descriptor\n",
    "    fast = cv2.xfeatures2d.StarDetector_create()\n",
    "    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "    # Detector\n",
    "    kp = fast.detect(img1,None)\n",
    "    # Descriptor\n",
    "    kp1, des1 = brief.compute(img1, kp)\n",
    "\n",
    "    # Detector\n",
    "    kp = fast.detect(img2,None)\n",
    "    # Descriptor\n",
    "    kp2, des2 = brief.compute(img2, kp)\n",
    "\n",
    "    # FLANN matching\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks = 50)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(np.float32(des1),np.float32(des2), k=2) # Use NP.FLOAT32 for ORB, BRIEF, etc\n",
    "\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good_matches = []\n",
    "    for m,n in matches:\n",
    "      if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "    \n",
    "    #if len(good_matches)>10:\n",
    "    p1 = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])#.reshape(-1,1,2)\n",
    "    p2 = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])#.reshape(-1,1,2)\n",
    "    \n",
    "    img3 = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None)\n",
    "    #if save:\n",
    "        #cv2.imwrite(path + \"_x_\"+filename2+\".jpg\", img3) #os.path.join(input_dir, 'im3.jpg'))\n",
    "    #cv2.imwrite(os.path.join(path, 'im2_x_im4.jpg'), img3)\n",
    "    cv2.imwrite(os.path.join(path, filename1 + \"_x_\" + filename2 + \".jpg\"), img3)\n",
    "\n",
    "    return p1, p2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- execute. \n",
    "#- PointMatchingAKAZE is the same function as Part 1.\n",
    "\n",
    "#pts0, pts1 = PointMatchingAKAZE(img0, img1, output_dir, images[i][:-4], images[i+1][:-4])\n",
    "pts0, pts1 = PointMatchingOpticalFlow(img0, img1, output_dir, images[i][:-4], images[i+1][:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>QUESTION! </b>  </div>\n",
    "\n",
    "- **Both the `Optical Flow` and `AKAZE` functions are available for feature detection and matching.** You should test both. **In no more than 75 words discuss the difference between these two algorithms, their major strengths and weaknesses and their typical usecases** _(when / where would you execute the solution)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. - Camera calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- same function as part 1.\n",
    "\n",
    "K = findCalibrationMat()\n",
    "posearr = K.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. b. Essential matrix and pose _(relative orientation)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- essentially the same function as Part 1. but given the nature of the solution [many images simultaeously] we need to create variables to \n",
    "#- accomodate the upcoming deluge\n",
    "\n",
    "#- define empty rotation matrices for each left-right image and fill them later\n",
    "R_t_0 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]])\n",
    "R_t_1 = np.empty((3, 4))\n",
    "\n",
    "#- set left image calibration - rotation\n",
    "P1 = np.matmul(K, R_t_0)\n",
    "Pref = P1\n",
    "P2 = np.empty((3, 4))\n",
    "\n",
    "#- the xyz and rgb for the .ply\n",
    "Xtot = np.zeros((1, 3))\n",
    "colorstot = np.zeros((1, 3))\n",
    "\n",
    "# Finding essential matrix\n",
    "E, mask = cv2.findEssentialMat(pts0, pts1, K, method=cv2.RANSAC, prob=0.999, threshold=0.4, mask=None)\n",
    "pts0 = pts0[mask.ravel() == 1]\n",
    "pts1 = pts1[mask.ravel() == 1]\n",
    "\n",
    "# The pose obtained is for second image with respect to first image\n",
    "_, R, t, mask = cv2.recoverPose(E, pts0, pts1, K)                   #- finding the pose\n",
    "pts0 = pts0[mask.ravel() > 0]\n",
    "pts1 = pts1[mask.ravel() > 0]\n",
    "R_t_1[:3, :3] = np.matmul(R, R_t_0[:3, :3])\n",
    "R_t_1[:3, 3] = R_t_0[:3, 3] + np.matmul(R_t_0[:3, :3], t.ravel())\n",
    "\n",
    "#- set right image calibration - rotation\n",
    "P2 = np.matmul(K, R_t_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. c. Triangulation\n",
    "\n",
    "Given two cameras, thier relative orientation (pose and geometry) and matching keypoints in both images we can project those points _from the 2D image into the 3D world_. At this stage; `Triangulation()` gives us a sparse 3D point cloud for one image pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function, for triangulation, given the image pair and their corresponding projection matrices\n",
    "def Triangulation(P1, P2, pts1, pts2, K, repeat):\n",
    "    if not repeat:\n",
    "        points1 = np.transpose(pts1)\n",
    "        points2 = np.transpose(pts2)\n",
    "    else:\n",
    "        points1 = pts1\n",
    "        points2 = pts2\n",
    "\n",
    "    cloud = cv2.triangulatePoints(P1, P2, points1, points2)\n",
    "    cloud = cloud / cloud[3]\n",
    "\n",
    "    return points1, points2, cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- execute\n",
    "\n",
    "# Triangulation is done for the first image pair. The poses will be set as reference and used for incremental SfM\n",
    "pts0, pts1, points_3d = Triangulation(P1, P2, pts0, pts1, K, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. c. i) Reprojection error\n",
    "\n",
    "The reprojection error referes to how well the 3D triangulated points, **_backprojected onto the image_**, compares with the original keypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation for Reprojection error in main pipeline\n",
    "def ReprojectionError(X, pts, Rt, K, homogenity):\n",
    "    total_error = 0\n",
    "    R = Rt[:3, :3]\n",
    "    t = Rt[:3, 3]\n",
    "\n",
    "    r, _ = cv2.Rodrigues(R)\n",
    "    if homogenity == 1:\n",
    "        X = cv2.convertPointsFromHomogeneous(X.T)\n",
    "\n",
    "    p, _ = cv2.projectPoints(X, r, t, K, distCoeffs=None)\n",
    "    p = p[:, 0, :]\n",
    "    p = np.float32(p)\n",
    "    pts = np.float32(pts)\n",
    "    if homogenity == 1:\n",
    "        total_error = cv2.norm(p, pts.T, cv2.NORM_L2)\n",
    "    else:\n",
    "        total_error = cv2.norm(p, pts, cv2.NORM_L2)\n",
    "    pts = pts.T\n",
    "    tot_error = total_error / len(p)\n",
    "    #print(p[0], pts[0])\n",
    "\n",
    "    return tot_error, X, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- execute\n",
    "\n",
    "# Backprojecting the 3D points onto the image and calculate the reprojection error. Ideally it should be less than one.\n",
    "error, points_3d, repro_pts = ReprojectionError(points_3d, pts1, R_t_1, K, homogenity = 1)\n",
    "print(\"REPROJECTION ERROR: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. d. Perspective-$n$-Point\n",
    "\n",
    "<img style=\"float:right;\" src=\"./img/pinhole_model.png\" width= 40% />\n",
    "<br><br>\n",
    "Perspective-$n$-Point refers to refining the pose of the camera giving the keypoints and their 3D projections.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for Perspective-n-Point\n",
    "def PnP(X, p, K, d, p_0, initial):\n",
    "    # print(X.shape, p.shape, p_0.shape)\n",
    "    if initial == 1:\n",
    "        X = X[:, 0, :]\n",
    "        p = p.T\n",
    "        p_0 = p_0.T\n",
    "\n",
    "    ret, rvecs, t, inliers = cv2.solvePnPRansac(X, p, K, d, cv2.SOLVEPNP_ITERATIVE)\n",
    "    # print(X.shape, p.shape, t, rvecs)\n",
    "    R, _ = cv2.Rodrigues(rvecs)\n",
    "\n",
    "    if inliers is not None:\n",
    "        p = p[inliers[:, 0]]\n",
    "        X = X[inliers[:, 0]]\n",
    "        p_0 = p_0[inliers[:, 0]]\n",
    "\n",
    "    return R, t, p, X, p_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- execute. notice we pass an empty rotation matrix to the PnP and get a Rot and trans back\n",
    "\n",
    "Rot, trans, pts1, points_3d, pts0t = PnP(points_3d, pts1, K, np.zeros((5, 1), dtype=np.float32), pts0, initial=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. e and f. Iteratively add new images and bundle adjustment.\n",
    "\n",
    "At this stage we have mostly completed the traditional stereo-photogrammetry process.  \n",
    "\n",
    "With **SfM** however we add more images and iteratively solve and refine.  \n",
    "\n",
    "As we add more data; we constantly look back at the previously created dataset _(keypoints, their 3D projections and the geometry and pose of the cameras)_ solve for their respective reprojection errors and perform a final optimisation via a **Bundle Adjustment**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BundleAdjustment(points_3d, temp2, Rtnew, K, r_error):\n",
    "\n",
    "\t# Set the Optimization variables to be optimized\n",
    "\topt_variables = np.hstack((Rtnew.ravel(), K.ravel()))\n",
    "\topt_variables = np.hstack((opt_variables, temp2.ravel()))\n",
    "\topt_variables = np.hstack((opt_variables, points_3d.ravel()))\n",
    "\n",
    "\terror = np.sum(OptimReprojectionError(opt_variables))\n",
    "\tcorrected_values = least_squares(fun = OptimReprojectionError, x0 = opt_variables, gtol = r_error)\n",
    "\n",
    "\tcorrected_values = corrected_values.x\n",
    "\tRt = corrected_values[0:12].reshape((3,4))\n",
    "\tK = corrected_values[12:21].reshape((3,3))\n",
    "\trest = len(corrected_values[21:])\n",
    "\trest = int(rest * 0.4)\n",
    "\tp = corrected_values[21:21 + rest].reshape((2, int(rest/2)))\n",
    "\tX = corrected_values[21 + rest:].reshape((int(len(corrected_values[21 + rest:])/3), 3))\n",
    "\tp = p.T\n",
    "\n",
    "\treturn X, p, Rt\n",
    "\n",
    "# Calculate reprojection error for bundle adjustment\n",
    "def OptimReprojectionError(x):\n",
    "\tRt = x[0:12].reshape((3,4))\n",
    "\tK = x[12:21].reshape((3,3))\n",
    "\trest = len(x[21:])\n",
    "\trest = int(rest * 0.4)\n",
    "\tp = x[21:21 + rest].reshape((2, int(rest/2)))\n",
    "\tX = x[21 + rest:].reshape((int(len(x[21 + rest:])/3), 3))\n",
    "\tR = Rt[:3, :3]\n",
    "\tt = Rt[:3, 3]\n",
    "\n",
    "\ttotal_error = 0\n",
    "\n",
    "\tp = p.T\n",
    "\tnum_pts = len(p)\n",
    "\terror = []\n",
    "\tr, _ = cv2.Rodrigues(R)\n",
    "\n",
    "\tp2d, _ = cv2.projectPoints(X, r, t, K, distCoeffs = None)\n",
    "\tp2d = p2d[:, 0, :]\n",
    "\t#print(p2d[0], p[0])\n",
    "\tfor idx in range(num_pts):\n",
    "\t\timg_pt = p[idx]\n",
    "\t\treprojected_pt = p2d[idx]\n",
    "\t\ter = (img_pt - reprojected_pt)**2\n",
    "\t\terror.append(er)\n",
    "\n",
    "\terr_arr = np.array(error).ravel()/num_pts\n",
    "\n",
    "\t#print(np.sum(err_arr))\n",
    "\t#err_arr = np.sum(err_arr)\n",
    "\n",
    "\treturn err_arr\n",
    "\n",
    "def common_points(pts1, pts2, pts3):\n",
    "    '''Here pts1 represent the keypoints in image 2 found during 1-2 matching\n",
    "    and pts2 are the keypoints in image 2 find during matching of 2-3 '''\n",
    "    indx1 = []\n",
    "    indx2 = []\n",
    "    for i in range(pts1.shape[0]):\n",
    "        a = np.where(pts2 == pts1[i, :])\n",
    "        if a[0].size == 0:\n",
    "            pass\n",
    "        else:\n",
    "            indx1.append(i)\n",
    "            indx2.append(a[0][0])\n",
    "\n",
    "    '''temp_array1 and temp_array2 which are not common '''\n",
    "    temp_array1 = np.ma.array(pts2, mask=False)\n",
    "    temp_array1.mask[indx2] = True\n",
    "    #temp_array1.mask[indx1] = True\n",
    "    temp_array1 = temp_array1.compressed()\n",
    "    temp_array1 = temp_array1.reshape(int(temp_array1.shape[0] / 2), 2)\n",
    "\n",
    "    temp_array2 = np.ma.array(pts3, mask=False)\n",
    "    temp_array2.mask[indx2] = True\n",
    "    temp_array2 = temp_array2.compressed()\n",
    "    temp_array2 = temp_array2.reshape(int(temp_array2.shape[0] / 2), 2)\n",
    "    #print(\"Shape New Array\", temp_array1.shape, temp_array2.shape)\n",
    "\n",
    "    return np.array(indx1), np.array(indx2), temp_array1, temp_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- loop. add a new image and repeat the process with an added bundle adjustment to optimise the overdetermined system\n",
    "\n",
    "#- gtol_thresh represents the gradient threshold or the min jump in update that can happen. If the jump is smaller, optimization is terminated.\n",
    "#- Note that most of the time, the pipeline yield a reprojection error less than 0.1! \n",
    "#- However, it is often too slow, often close to half a minute per frame!\n",
    "gtol_thresh = 0.5\n",
    "\n",
    "for i in tqdm(range(tot_imgs)):\n",
    "    # Acquire new image to be added to the pipeline and acquire matches with image pair\n",
    "    img2 = cv2.imread(os.path.join(input_dir, images[i + 2]))\n",
    "    # pts0,pts1 = find_features(img1,img2)\n",
    "\n",
    "    #pts_, pts2 = PointMatchingAKAZE(img1, img2, output_dir, images[i + 1][:-4], images[-c][:-4])\n",
    "    pts_, pts2 = PointMatchingOpticalFlow(img1, img2, output_dir, images[i + 1][:-4], images[-c][:-4])\n",
    "    if i != 0:\n",
    "        pts0, pts1, points_3d = Triangulation(P1, P2, pts0, pts1, K, repeat = False)\n",
    "        pts1 = pts1.T\n",
    "        points_3d = cv2.convertPointsFromHomogeneous(points_3d.T)\n",
    "        points_3d = points_3d[:, 0, :]\n",
    "\n",
    "    # There are going to be some common point in pts1 (previous pair) and pts_ (this pair) we need to find the indx1 of pts1 match with indx2 in pts_\n",
    "    indx1, indx2, temp1, temp2 = common_points(pts1, pts_, pts2)\n",
    "    com_pts2 = pts2[indx2]\n",
    "    com_pts_ = pts_[indx2]\n",
    "    com_pts0 = pts0.T[indx1]\n",
    "    #- We have the 3D - 2D correspondence for the new image as well as the previous point cloud. \n",
    "    #- The common points can be used to find the world coordinates of the new image\n",
    "    #- using Perspective - n - Point (PnP)\n",
    "    Rot, trans, com_pts2, points_3d, com_pts_ = PnP(points_3d[indx1], com_pts2, K, np.zeros((5, 1), dtype=np.float32), com_pts_, initial = 0)\n",
    "    # Find the equivalent projection matrix for new image\n",
    "    Rtnew = np.hstack((Rot, trans))\n",
    "    Pnew = np.matmul(K, Rtnew)\n",
    "\n",
    "    #print(Rtnew)\n",
    "    error, points_3d, _ = ReprojectionError(points_3d, com_pts2, Rtnew, K, homogenity = 0)\n",
    "\n",
    "    temp1, temp2, points_3d = Triangulation(P2, Pnew, temp1, temp2, K, repeat = False)\n",
    "    error, points_3d, _ = ReprojectionError(points_3d, temp2, Rtnew, K, homogenity = 1)\n",
    "    print(\"Reprojection Error: \", error)\n",
    "    \n",
    "    # We are storing the pose for each image. \n",
    "    posearr = np.hstack((posearr, Pnew.ravel()))\n",
    "\n",
    "    print(\"Bundle Adjustment...\")\n",
    "    points_3d, temp2, Rtnew = BundleAdjustment(points_3d, temp2, Rtnew, K, gtol_thresh)\n",
    "    Pnew = np.matmul(K, Rtnew)\n",
    "    error, points_3d, _ = ReprojectionError(points_3d, temp2, Rtnew, K, homogenity = 0)\n",
    "    print(\"Minimized error: \",error)\n",
    "    \n",
    "    Xtot = np.vstack((Xtot, points_3d))\n",
    "    pts1_reg = np.array(temp2, dtype=np.int32)\n",
    "    colors = np.array([img2[l[1], l[0]] for l in pts1_reg])\n",
    "    colorstot = np.vstack((colorstot, colors))\n",
    "\n",
    "    R_t_0 = np.copy(R_t_1)\n",
    "    P1 = np.copy(P2)\n",
    "    plt.pause(0.05)\n",
    "\n",
    "    c = c - 1\n",
    "    img0 = np.copy(img1)\n",
    "    img1 = np.copy(img2)\n",
    "    pts0 = np.copy(pts_)\n",
    "    pts1 = np.copy(pts2)\n",
    "    #P1 = np.copy(P2)\n",
    "    P2 = np.copy(Pnew)\n",
    "    #cv2.imshow('image', img2)\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- save to sfm.ply and pose.txt\n",
    "\n",
    "def to_ply(path, point_cloud, colors):#, densify):\n",
    "    out_points = point_cloud.reshape(-1, 3) * 200\n",
    "    out_colors = colors.reshape(-1, 3)\n",
    "    #print(out_colors.shape, out_points.shape)\n",
    "    verts = np.hstack([out_points, out_colors])\n",
    "\n",
    "    # cleaning point cloud\n",
    "    mean = np.mean(verts[:, :3], axis=0)\n",
    "    temp = verts[:, :3] - mean\n",
    "    dist = np.sqrt(temp[:, 0] ** 2 + temp[:, 1] ** 2 + temp[:, 2] ** 2)\n",
    "    #print(dist.shape, np.mean(dist))\n",
    "    indx = np.where(dist < np.mean(dist) + 300)\n",
    "    verts = verts[indx]\n",
    "    #print( verts.shape)\n",
    "    ply_header = '''ply\n",
    "\t\tformat ascii 1.0\n",
    "\t\telement vertex %(vert_num)d\n",
    "\t\tproperty float x\n",
    "\t\tproperty float y\n",
    "\t\tproperty float z\n",
    "\t\tproperty uchar blue\n",
    "\t\tproperty uchar green\n",
    "\t\tproperty uchar red\n",
    "\t\tend_header\n",
    "\t\t'''\n",
    "    #else:\n",
    "    with open(os.path.join(path, 'akaze_sfm.ply'), 'w') as f: #- change the name of the .ply\n",
    "        f.write(ply_header % dict(vert_num=len(verts)))\n",
    "        np.savetxt(f, verts, '%f %f %f %d %d %d')\n",
    "\n",
    "\n",
    "#- \n",
    "print(\"Processing Point Cloud...\")\n",
    "print(Xtot.shape, colorstot.shape)\n",
    "to_ply(output_dir, Xtot, colorstot)#, densify)\n",
    "print(\"Done!\")\n",
    "# Saving projection matrices for all the images. #- change the name of the .txt\n",
    "np.savetxt(os.path.join(output_dir, 'akaze_pose.csv'), posearr, delimiter = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>TASK! / QUESTION!</b>  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **This exercise executes an _iterative_ SfM pipeline. _Global_ solutions also exist. How do these differ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Capture several photographs of any object** (no more than 15) **and execute this exercise. You will need to create a `calib.txt`.**\n",
    "\n",
    "  **In no more than 50 words; discuss the result** _(the *sfm.ply)_. **You are expected to not only compare the `Optical Flow` and `Akaze` solutions but comment on the quality, usefulness and how the solution can be improved**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqn_w8mroLkd"
   },
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "  <strong>REQUIRED!</strong> \n",
    "  \n",
    "Don't forget. Besides the contents on this Notebook your project folders need to be submitted also.  \n",
    "You are expected to submit both a `opticalFlow_sfm.ply` and an `akaze_sfm.ply`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdzwQCsMoLnN"
   },
   "source": [
    "_images:_\n",
    "\n",
    "- **pinhole model**: https://kornia.readthedocs.io/en/latest/geometry.camera.pinhole.html\n",
    "- **epipolar geometry**: https://cw.fel.cvut.cz/b181/_media/courses/b3b33vir/exteroceptive_sensors.pdf\n",
    "- **disparity**: https://www.e-consystems.com/blog/camera/technology/what-is-a-stereo-vision-camera-2/\n",
    "- **global sfm**: http://theia-sfm.org/sfm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eLz9rH3oLp8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
