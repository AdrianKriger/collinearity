{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq2o9EST11yp"
   },
   "source": [
    "# Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <strong> </strong> [ Write your surname.name here (like that surname.name) ]  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iemMW2yH1XIs"
   },
   "source": [
    "#### Welcome to this exercise which introduces Computer Vision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWmnjDZB2gxZ"
   },
   "source": [
    "<img style=\"float:right;\" src=\"./img/pinhole_model.png\" width= 50% />\n",
    "\n",
    "<br>\n",
    "While photogrammetry has over 100 years of tradition, the past several years has seen Computer Vision applied in the geospatial domain.\n",
    "<br> <br>\n",
    "\n",
    "\n",
    "Newer technologies often come with their own jargon (terms, definitions and language). \n",
    "\n",
    "This exercise is not meant to be exhaustive but serves as a gentle introduction to the concepts and potential of Computer Vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lBqbY1r2g1h"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> <strong> Lets first define the core difference between these two methods. </strong>  <br> <br>\n",
    "\n",
    "**Computer Vision**: interpretation and understanding of visual data often involving real-time decision-making by machines. Enable machines to interpret and understand visual data. Medical imaging and autonomous navigation|<br><br>\n",
    "**Photogrammetry**: reconstruct 3D structures and spatial relationships. Accurately measure and reconstruct the three-dimensional shape. Remote sensing and mapping.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order for realise the goal of Computer Vision it is often necessary to harvest 3D data from imagery**. While this is also the aim of photogrammetry; the processes differ slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Aspect|Traditional Photogrammetry|Computer Vision|\n",
    "|---|---|---|\n",
    "|Stereo|Primarily relies on **_stereo matching_** to create 3D models|Combines stereo vision with **_advanced techniques_** like Structure from Motion Multi-view Stereo (SfM-MVS) and SLAM (Simultaneous Localization and Mapping)|\n",
    "|Calibration|**_Precise sensor calibration_** for accurate results|May involve more **_automated_** calibration procedures|\n",
    "|Processing Speed|Time-consuming manual processes|Often performs real-time or **_near-real-time processing_** thanks to powerful hardware and **_optimized algorithms_**.|\n",
    "|Data Volume|Deals with smaller datasets of images.|Analyzes large datasets of images, videos, and 3D point clouds.|\n",
    "|Data Availability|Data acquisition and processing may be costly and time-consuming.|Access to data is becoming more abundant and affordable, thanks to the proliferation of digital cameras and sensors|\n",
    "|Accuracy|High accuracy _(or rather the **confidence** in the quality)_ making it suitable for geospatial applications|Accuracy varies depending on the application and the quality of data and algorithms|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBq0MF5L2g5J"
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "  <strong>REQUIRED!</strong> \n",
    "  \n",
    "You are required to insert your outputs and any comment into this document. \n",
    "    \n",
    "The **aim** of this exercise is for you to execute this Notebook on **_a series of photographs you have captured yourself_**. The document you submit should therefore contain the existing text in addition to:   \n",
    "        \n",
    "\n",
    " - Plots and other outputs from exec the code cells  \n",
    " - Discussion of your plots and other outputs as well as conclusions reached.  \n",
    " - This should also include any hypotheses and assumptions made as well as factors that may affect your conclusions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLOmuPSmzwTx"
   },
   "outputs": [],
   "source": [
    "#- load the magic\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxXhXIVxvWIh",
    "outputId": "c24ac9c4-b165-4f59-c172-c587850cf492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ov2ly6SsxS3Q"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "dir = os.path.join(\"drive/My Drive/UCTJuly-Nov2023/APG3012S-RSandP/assignments/collinearity/data/sfm/\")#data/sfm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <strong> For this exercise we will harvest 3D data from imagery with Computer Vision. </strong>  \n",
    "<br><br>\n",
    "We will do so in two stages. \n",
    "<br><br>  \n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**1. Traditional stereo-vision** _(two photographs)_  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**2. Structure-from-Motion (SfM)** _(with several images to compare with traditional photogrammetry)_\n",
    "<!-- &nbsp;&nbsp;&nbsp;&nbsp;**2.** extend the SfM process with **Multi-view Stereo (MVS)** _(and show how the method accomodates many images)_-->\n",
    "\n",
    "In the process we will work through such concepts as Feature Matching, Essential and Fundamental matrices and Disparity\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_4FeaOlkkLwl"
   },
   "outputs": [],
   "source": [
    "#- path\n",
    "input_dir = os.path.join(dir, 'stereo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stereo-vision\n",
    "\n",
    "Before we _really_ get into **Computer Vision**; it might be helpful to introduce modern _'algorithm-based'_ stereo photogrammetry. Modern **Computer Vision** is infact already very mature with completely automated workflows that harvest 3D information via Deep Learning _(neural networks)_. \n",
    "\n",
    "We therefore take a step back and start with the basics to understand how these procedures execute a solution. Two images, of the same feature, taken from two different viewpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"./data/stereo/im0.png\" alt=\"Drawing\" style=\"width: 550px;\"/> </td>\n",
    "<td> <img src=\"./data/stereo/im0.png\" alt=\"Drawing\" style=\"width: 550px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. a. Feature Matching\n",
    "\n",
    "In order to harvest 3D data we need the location and orientation of the cameras; at the moment the image was captured. \n",
    "\n",
    "Like traditional **Photogrammerty** which needs _**some**_ variables _(ground control, location and orientation of photographs)_ to solve for the **collinearity equation**; **Computer Vision** also needs a few known variables. Even so; modern **Computer Vision** can recover the geometry and pose of the camera _(the relative orientation)_ from the imagery itself. It can do so through feature detection and matching algorithms which work in two steps:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;i) find features _(known as **keypoints**)_ in images and  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;ii) **match** corresponding features _(connect the features that match and disgard those that don't)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "6H9f-UnTnzMV"
   },
   "outputs": [],
   "source": [
    "#- feature matching\n",
    "def PointMatchingAKAZE(img1, img2, path):\n",
    "\n",
    "    akaze = cv2.AKAZE_create()\n",
    "    kp1, des1 = akaze.detectAndCompute(img1, None)\n",
    "    kp2, des2 = akaze.detectAndCompute(img2, None)\n",
    "\n",
    "    #- match features\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    #- ratio test\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.70 * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "\n",
    "    #- cv2.drawMatchesKnn expects list of lists as matches.\n",
    "    #img3 = cv2.drawMatches(img1, kp1, img2, kp2, good[:50], None, flags=2)\n",
    "    img3 = cv2.drawMatches(img1, kp1, img2, kp2, good, None, flags=2)\n",
    "    cv2.imwrite(os.path.join(path, 'imL_x_imR.jpg'), img3)\n",
    "\n",
    "    return pts1, pts2, good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**keypoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/stereo/keypoints.jpg\" alt=\"Drawing\" style=\"width: 1000px;\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**matches**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/stereo/im2_x_im4.jpg\" alt=\"Drawing\" style=\"width: 1000px;\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>TASK! </b>  </div>\n",
    "\n",
    "- **You are expected to insert the result** _(the `keypoints` and `..._x_....jpg`)_ **of your own dataset in the `cells`** _(double-click the image and change the file)_ **above and discuss the result.** \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>HINT!</b> Open the image in another application. Zoom and look at the number of features and the respective matches. </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7WYjxQzZn3Ak"
   },
   "outputs": [],
   "source": [
    "#- do (fix) this\n",
    "def PointMatchingSURF(img1, img2, save = False, path = None):\n",
    "    #Match with SURF\n",
    "    surf = cv2.xfeatures2d.SURF_create()\n",
    "    keypoints1, descriptors1 = surf.detectAndCompute(img1, None)\n",
    "    keypoints2, descriptors2 = surf.detectAndCompute(img2, None)\n",
    "\n",
    "    img4 = cv2.drawKeypoints(img1, keypoints1, None)\n",
    "\n",
    "    cv2.imwrite(\"KP.jpg\", img4)\n",
    "    bf = cv2.BFMatcher_create()\n",
    "\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    img3 = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[:200], None, flags=2)\n",
    "\n",
    "    if save:\n",
    "        cv2.imwrite('03' + \"_x_\" + '04' +\".jpg\", img3)\n",
    "\n",
    "    return keypoints1, keypoints2,matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>QUESTION! </b>  </div>\n",
    "\n",
    "- **This exercise does not use `Scale-invariant feature transform (SIFT)` to match features but executes the `AKAZE` algorithm** _(you are expected to change the `code` and test both solutions)_.\n",
    "  \n",
    "    **Discuss the difference between these two methods. Mention their strenghts and weaknessess. A note about intellectual property is expected. Your answer cannot be more than 150 words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. b. Camera calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While we can calibrate a camera with `opencv` we use existing calibration information.** \n",
    "\n",
    "You will need to create your own for the **TASK** at the end of this exercise so lets see what it contains.  The `intrinsic.txt` is structured:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "  fx       & 0   & cx   \\\\\n",
    "  0       & fy   & cy   \\\\\n",
    "  0       & 0   & 1     \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "where `fx = f * resX (the number of columns) / sensorSizeX` and `fy = f * resY (the number of rows) / sensorSizeY` with `f = focal length`. \n",
    "`cx` and `cy` represent the principle point of the image and, with modern digital cameras, can be calculated with `cx = resX (the number of columns) / 2` and `cy = resY (the number of rows) / 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bJqr3Dl7l6Ab"
   },
   "outputs": [],
   "source": [
    "#- fix this\n",
    "def findCalibrationMat():\n",
    "    #with open(input_dir+'intrinsic.txt') as f: #os.path.join(input_dir, 'im3.jpg'))\n",
    "    with open(os.path.join(dir, 'intrinsic.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "    return np.array(\n",
    "        [l.strip().split(' ') for l in lines],\n",
    "        dtype=np.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. c. Fundamental and Essential matrices _(relative orientation)_\n",
    "\n",
    "\n",
    "In **Photogrammetry** we locate and orient _**one**_ image and are able to calculate a 3D coordinate via the **collinearity equation**. When we do intersect **image rays** from more than one image; we do so from already oriented imagery. \n",
    "\n",
    "In order to harvest 3D information; we need to know the relationship between the images (_i.e.: the relative orientation)_.\n",
    "\n",
    "In Computer Vision we call this the **essential and fundamental matrices**. These matrices solve for the geometry and pose of the cameras at the moment the images were captured.\n",
    "\n",
    "The previous step (feature matching) in **Computer Vision** thus serves two purposes. We recover the _relative orientation_ of all images simultaneously (essential matrix) and describe the relationship between images in the same scene. And use these to map points of one image to lines in another (fundamental matrix). \n",
    "\n",
    "Think of an **essential and fundamental matrices** as generalizations of the **collinearity equations** that account for the relative geometry between multiple cameras. They allow for the reconstruction of 3D structures by considering the relationship between corresponding points in multiple cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "efrPSg3Gn7CN"
   },
   "outputs": [],
   "source": [
    "def FindEssentialMat(kp1, kp2, matches, K):\n",
    "    \n",
    "    #- essential matrix\n",
    "    E, mask = cv2.findEssentialMat(kp1, kp2, K, method = cv2.RANSAC, prob = 0.999, threshold = 0.4, mask = None)\n",
    "    kp1 = kp1[mask.ravel() ==1]\n",
    "    kp2 = kp2[mask.ravel() ==1]\n",
    "    #- fundamental matrix\n",
    "    #fundamental_matrix, inliers = cv.findFundamentalMat(kp1, kp2, cv.FM_RANSAC)\n",
    "    \n",
    "    #- essential matrix to rotation and translation\n",
    "    _, R, t, mask = cv2.recoverPose(E, kp1, kp2, K)\n",
    "    \n",
    "    return E, R, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>EXTRA!</b> </div> \n",
    "\n",
    "- **While beyond the scope of this exercise; the overal aim of the Fundamental and Essential matrices is to recover the Epipolar Geometry and the Epipolar Line. These _narrow_ the search space to recovery the Disparity** _(the horizontal difference)_ **between pixels.**\n",
    "\n",
    "  **You are welcome to write 100 words on Epipolar Geometry and Epipolar Line and what it does. The choice is yours. Its up to you**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. d. Disparity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:right;\" src=\"./img/Stereo-Disparity-in-Cameras.png\" width= 40% />\n",
    "\n",
    "<br>\n",
    "\n",
    "We see **Disparity** and _perceive depth_. \n",
    "\n",
    "Choose an object 5-meters away from you. Close your left eye... open your left eye. Now close you right eye. Alternate. The object will appear to move _(horizontally)_. Left and right. \n",
    "\n",
    "We see _**horizontal displacement**_ and interpret the phenomenon as _3D vision_.\n",
    "\n",
    "Modern **Computer Vision** harvests 3D information in _exactly the same_ way as the human eye sees. \n",
    "\n",
    "Because we have correctly oriented photographs we know the _perceived shift in location_ of any feature. We use this knowledge to create a depth _(disparity)_ map that decribes the distance from the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dMjONpAcoH1E"
   },
   "outputs": [],
   "source": [
    "def rectifyToDisparity(imgL, imgR, K, D, R, t, path):\n",
    "    \n",
    "    R1, R2, P1, P2, Q, a, b = cv2.stereoRectify(K, D, K, D, (2964, 2000), R, t)\n",
    "    map1, map2 = cv2.initUndistortRectifyMap(K, D, R1, P1, (2964, 2000), cv2.CV_16SC2)\n",
    "    imgLrec = cv2.remap(imgL, map1, map2, cv2.INTER_CUBIC)\n",
    "    #cv2.imwrite(os.path.join(path, 'imgLrectfy.jpg'), imgLrec)\n",
    "    \n",
    "    map3, map4 = cv2.initUndistortRectifyMap(K, D, R2, P2, (2964, 2000), cv2.CV_16SC2)\n",
    "    imgRrec = cv2.remap(imgR, map3, map4, cv2.INTER_CUBIC)\n",
    "    #cv2.imwrite(os.path.join(path, 'imgRrectfy.jpg'), imgRrec)\n",
    "    \n",
    "    max_disparity = 199\n",
    "    min_disparity = 23\n",
    "    num_disparities = max_disparity - min_disparity\n",
    "    window_size = 5\n",
    "    stereo = cv2.StereoSGBM_create(minDisparity = min_disparity, numDisparities = num_disparities, blockSize = 5,\n",
    "                                 uniquenessRatio = 5, speckleWindowSize = 5, speckleRange = 5, disp12MaxDiff = 2,\n",
    "                                 P1 = 8*3*window_size**2, P2 = 32*3*window_size**2)\n",
    "    \n",
    "    stereo2 = cv2.ximgproc.createRightMatcher(stereo)\n",
    "    \n",
    "    lamb = 8000\n",
    "    sig = 1.5\n",
    "    visual_multiplier = 1.0\n",
    "    wls_filter = cv2.ximgproc.createDisparityWLSFilter(stereo)\n",
    "    wls_filter.setLambda(lamb)\n",
    "    wls_filter.setSigmaColor(sig)\n",
    "    \n",
    "    disparity = stereo.compute(imgLrec, imgRrec)\n",
    "    disparity2 = stereo2.compute(imgRrec, imgLrec)\n",
    "    disparity2 = np.int16(disparity2)\n",
    "    \n",
    "    filteredImg = wls_filter.filter(disparity, imgL, None, disparity2)\n",
    "    _, filteredImg = cv2.threshold(filteredImg, 0, max_disparity * 16, cv2.THRESH_TOZERO)\n",
    "    filteredImg = (filteredImg / 16).astype(np.uint8)\n",
    "    cv2.imwrite(os.path.join(path, 'dsprty.jpg'), filteredImg)\n",
    "    \n",
    "    return filteredImg#, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disparity image**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/stereo/dsprty.jpg\" alt=\"Drawing\" style=\"width: 1000px;\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>QUESTION! </b>  </div>\n",
    "\n",
    "- **Why is the structure of a Disparity image** _(every pixel is a z)_ **exactly the same as a Digital Elevation Model (DEM)? What does this say about what we have just done?**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>BONUS!</b> Change the colour palette from gray-scale. </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. e. Project to 3D\n",
    "\n",
    "We are now at the final stage of the stereo-vision process. We want to harvest the values, from the **disparity** map, and convert the _depth_ into 3D information _(a point cloud we can render in a 3D environment)_.\n",
    "\n",
    "In order to do so we need additional information. We need the distance _(the baseline)_ between the cameras (the eyes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = 193.001/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "07757nREoLK8"
   },
   "outputs": [],
   "source": [
    "def Reprojection3D(image, disparity, f, b):\n",
    "    #- Q. you might need to... \n",
    "    Q = np.array([[1, 0, 0, -2964/2], [0, 1, 0, -2000/2],[0, 0, 0, f],[0, 0, -1/b, -124.343/b]])\n",
    "    \n",
    "    points = cv2.reprojectImageTo3D(disparity, Q)\n",
    "    mask = disparity > disparity.min()\n",
    "    colors = image\n",
    "    \n",
    "    out_points = points[mask]\n",
    "    out_colors = image[mask]\n",
    "    \n",
    "    verts = out_points.reshape(-1,3)\n",
    "    colors = out_colors.reshape(-1,3)\n",
    "    verts = np.hstack([verts, colors])\n",
    "    \n",
    "    ply_header = '''ply\n",
    "        format ascii 1.0\n",
    "        element vertex %(vert_num)d\n",
    "        property float x\n",
    "        property float y\n",
    "        property float z\n",
    "        property uchar blue\n",
    "        property uchar green\n",
    "        property uchar red\n",
    "        end_header\n",
    "        '''\n",
    "    \n",
    "    with open(input_dir + '/stereo.ply', 'w') as f:\n",
    "        f.write(ply_header %dict(vert_num = len(verts)))\n",
    "        np.savetxt(f, verts, '%f %f %f %d %d %d')\n",
    "        \n",
    "def StereoCV():\n",
    "    #- the function\n",
    "    img1 = cv2.imread(os.path.join(input_dir, 'im0.png'))\n",
    "    img2 = cv2.imread(os.path.join(input_dir, 'im1.png'))\n",
    "\n",
    "    pts1, pts2, matches = PointMatchingAKAZE(img1, img2, path = input_dir)\n",
    "\n",
    "    #K = findCalibrationMat()\n",
    "    K = np.array([[3979.911, 0, 1369.115], [0, 3979.911, 1019.507], [0, 0, 1]], dtype = np.float32)\n",
    "    D = np.zeros((5,1), dtype = np.float32)\n",
    "\n",
    "    E, R, t = FindEssentialMat(pts1, pts2, matches, K)\n",
    "\n",
    "    P1 = np.zeros((3,4))\n",
    "    P1 = np.matmul(K, P1)\n",
    "    P2 = np.hstack((R, t))\n",
    "    P2 = np.matmul(K, P2)\n",
    "\n",
    "    #filteredImg, Q = rectifyToDisparity(img1, img2, K, D, (2964, 2000), R, t, input_dir)\n",
    "    filteredImg, Q = rectifyToDisparity(img1, img2, K, D, R, t, input_dir)\n",
    "\n",
    "    f = 3979.911/2\n",
    "    #Reprojection3D(img1, filteredImg, Q)\n",
    "    Reprojection3D(img1, filteredImg, f, baseline)\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sA8F0iPfoLN0"
   },
   "outputs": [],
   "source": [
    "#- execute\n",
    "\n",
    "StereoCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/stereo/result.gif\" alt=\"Drawing\" style=\"width: 1000px;\"/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>TASK! </b>  </div>\n",
    "\n",
    "- **Capture two photographs of any object and execute this exercise. You will need to determine the distance between the cameras** _(baseline)_ **and create a `calib.txt`.**\n",
    "\n",
    "   **In no more than 50 words; discuss the result** _(the stereo.ply)_. **You are expected to comment on how the solution can be improved**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ click in this cell and write your answer here }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGsphZNOoLWM"
   },
   "source": [
    "## 2. Structure-from-Motion (SfM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-fxjgmzoLZE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will work with the well known [(Christoph Strecha's) Fountain-P11 dataset](https://certis.enpc.fr/demos/stereo/Data/Fountain11/index.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. c. Fundamental and Essential matrices _(location, scale and rotation)_\n",
    "\n",
    "\n",
    "In **Photogrammetry** we locate and orient _**one**_ image and are able to calculate a 3D coordinate via the **collinearity equation**. When we do intersect **image rays** from more than one image; we do so from already oriented imagery. \n",
    "\n",
    "With **Computer Vision** we **match** corresponding features in photographs. We still resolve for the geometry and pose _(relative orientation)_ of the camera but we do so for _**all cameras simultaneously**_ (more than one at a time). And because our focus is an automated process we also look to harvest 3D data via algorithms. \n",
    "\n",
    "The feature matching in **Computer Vision** thus serves two purposes. We recover the _relative orientation_ of all images simultaneously (essential matrix) and describe the relationship between images in the same scene. And use these to map points of one image to lines in another (fundamental matrix). \n",
    "\n",
    "Think of an **essential and fundamental matrices** as generalizations of the **collinearity equations** that account for the relative geometry between multiple cameras. They allow for the reconstruction of 3D structures by considering the relationship between corresponding points in multiple cameras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_e6poPRfoLes"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJJPKqBVoLb9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3IoT2aDoLhk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>TASK / QUESTION! </b>  </div>\n",
    "\n",
    "- **Execute this NoteBook on a series of photographs you have captured yourself** _(no more than 15)_. **This Notebook must therefore contain the output from you own dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqn_w8mroLkd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdzwQCsMoLnN"
   },
   "source": [
    "_images:_\n",
    "\n",
    "- **pinhole model**: https://kornia.readthedocs.io/en/latest/geometry.camera.pinhole.html\n",
    "- **disparity**: https://www.e-consystems.com/blog/camera/technology/what-is-a-stereo-vision-camera-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eLz9rH3oLp8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
